---
title: "JH Data Science Capstone - Milestone Report"
author: "Max Gribov"
date: "September 29, 2018"
output: 
  html_document:
    highlight: pygments  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary

The goal of the capstone project is to build a predictive text application. The application will use the words user typed to provide a suggestion for the next word. Number of suggested words can be a tuneable parameter, with 5 as default. The application will suggest the words by using an algorithm/model based on statistical properties of the training data.

Specifically, n-gram statistics will be used. N-Gram can be defined as a sequence of N words in their original order from the text. For example, "can be" is a bi-gram, "how do i" is a tri-gram. The metric of how often that particular n-gram occurs can be used to order these n-grams by their prevalence. 

The milestone report explores the word and n-gram statistics of the training data, outlines the suggestion algorithm, and outlines some potential future improvements.


## Training data exploration

The training data for this project contains a large number of blog posts, tweets, and news in English.
Linux wc command shows there are 4269678 total lines and 102073612 total words in all 3 files combined.
```{r, engine = 'bash', eval = FALSE}
wc -w -l *.txt
  # lines  words    file  
   899288  37334117 en_US.blogs.txt
  1010242  34365936 en_US.news.txt
  2360148  30373559 en_US.twitter.txt
  4269678 102073612 total
```


The dataset is very large, so we will use only 5% of the total content to train the model.
This was done by using the last 5% of the lines in each of the files, also via linux command line.
```{r, engine = 'bash', eval = FALSE}
tail -44964 en_US.blogs.txt > smaller/en_US.blogs-5percent.txt
tail -50512 en_US.news.txt > smaller/en_US.news-5percent.txt
tail -118007 en_US.twitter.txt > smaller/en_US.twitter-5percent.txt   
```


The first task is to import and clean the data, which includes stripping out invalid characters, removing profanity, and removing punctuation.
```{r getdata, eval=FALSE, message=FALSE, warning=FALSE}
library(tm)
library(stringi)
library(qdap)

# initial load and cleaning of corpus
# 5% of lines from each dataset
docs <- Corpus(DirSource('~/code/jh-datasci-projects/capstone/final/en_US/smaller'))

# standard cleaning
removeUnicode <- function(x) stri_replace_all_regex(x,"[^\x20-\x7E]","")
docs <- tm_map(docs, content_transformer(removeUnicode))

docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, tolower)

docs <- tm_map(docs, content_transformer(replace_contraction))

# source: http://www.cs.cmu.edu/~biglou/resources/bad-words.txt
profanity = readLines('~/code/jh-datasci-projects/capstone/bad-words.txt')
docs <- tm_map(docs, removeWords, profanity)

writeCorpus(docs, path="~/code/jh-datasci-projects/capstone/corpus")
```


Next, we collect basic corpus statistics: corpus size, most common words, most common n-grams, and their distributions.
```{r corpus_grams, results='asis', message=FALSE, warning=FALSE}
library(corpus)
library(tm)

docs <- Corpus(DirSource('~/code/jh-datasci-projects/capstone/corpus'))

unigrams <- term_stats(docs, ngrams = 1:1)
bigrams <- term_stats(docs, ngrams = 2:2)
trigrams <- term_stats(docs, ngrams = 3:3)
quadgrams <- term_stats(docs, ngrams = 4:4)

# all words
all_unique_words <- dim(unigrams)[1]
all_words <- sum(unigrams$count)

cat(paste("Total Number of Words:", all_words, ", Number of Unique Words - Vocabulary:", all_unique_words))

```


#### Frequency distribution of the n-grams (up to n=4) shows exponential decay, conforming to Zipf's law.
```{r gram_plots_freq, echo=FALSE, message=FALSE, warning=FALSE}
par(mfrow=c(2,2))
plot(unigrams[0:100, 'count'], type="l", main="Top 100 Unigram Frequency", ylab="Frequency", xlab="Term Rank")
plot(bigrams[0:100, 'count'], type="l", main="Top 100 Bigram Frequency", ylab="Frequency", xlab="Term Rank")
plot(trigrams[0:100, 'count'], type="l", main="Top 100 Trigram Frequency", ylab="Frequency", xlab="Term Rank")
plot(quadgrams[0:100, 'count'], type="l", main="Top 100 Quadgram Frequency", ylab="Frequency", xlab="Term Rank")
```


#### Most popular words (unigrams) and other n-grams (up to n=4).
```{r gram_plots, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)

top_unigrams <- ggplot(unigrams[0:10,], aes(x = reorder(term, count), y = count))
top_unigrams <- top_unigrams + geom_bar(stat = "identity") + coord_flip()  + labs(title = "Top 10 Unigrams")

top_bigrams <- ggplot(bigrams[0:10,], aes(x = reorder(term, count), y = count))
top_bigrams <- top_bigrams + geom_bar(stat = "identity") + coord_flip()  + labs(title = "Top 10 Bigrams")

top_trigrams <- ggplot(trigrams[0:10,], aes(x = reorder(term, count), y = count))
top_trigrams <- top_trigrams + geom_bar(stat = "identity") + coord_flip()  + labs(title = "Top 10 Trigrams")

top_quadgrams <- ggplot(quadgrams[0:10,], aes(x = reorder(term, count), y = count))
top_quadgrams <- top_quadgrams + geom_bar(stat = "identity") + coord_flip() + labs(title = "Top 10 Quadgrams")

top_unigrams
top_bigrams
top_trigrams
top_quadgrams
```


## Prediction algorithm

My initial approach is to simply use the counts of various n-grams to produce next suggested word.

The algorithm would return a specified number of suggested words, ranked by "best suggestion", with default of 5 words.

User's input would be truncated to maximum of last 3 words, and the next word would be suggested by looking for most common last word in the quadgram data. If no matches, or not enough matches are found, the input would be truncated again to last 2 words, and trigram data would be used, and so on, until the algorithm will return most common unigrams (single words). 

So for example, based on the quadgram plot above, if user enters "for the first", the next top suggested word based on quadgram data would be "time". 

If there are not enough quadgrams to find required number of suggested words, the user's input would be truncated again to "the first", and trigram data would be searched for most popular trigrams which start with "the first", and so on to bigram and unigram data if needed.


## Measuring Performance

To measure the accuracy of various suggesion algorithms and their improvements, we can take a number of n-grams from part of the corpus which we did not use for training, and use the algorithm to predict the last word of each sequence. Number of correct versus incorrect predictions can be used as a simple accuracy metric.


## Potential future improvements

Using a more sophisticated metric for suggessions than simple occurance count, for example Maximum Likelihood Estimation.

Considering "end of sentence" states in user's input to properly control how many n-grams to use for search.

Using Part Of Speech tagging to make decisions, so how likely are nouns to be followed by verbs versus adjectives for example.

Exploring if its possible to use Recurring Neural Networks, specifically Long Short Term Memory networks, to predict the next word: https://arxiv.org/abs/1703.10724



